# Safe 12GB GPU Configuration for MAMBA-TrackingBERT
# Guaranteed to fit in 12GB with room to spare

data:
  data_dir: "data/processed"
  train_split: 0.8
  val_split: 0.1
  test_split: 0.1
  batch_size: 512         # Conservative but much larger than original 128
  num_workers: 16         # Good CPU utilization
  pin_memory: true
  track_length_range: [4, 8]
  volumes: [7, 8, 9]
  max_tracks_per_event: 12000

model:
  vocab_size: 20000
  pad_token_id: 0
  mask_token_id: 1
  
  # Medium MAMBA architecture - fits comfortably in 12GB
  d_model: 192           # Sweet spot for 12GB
  d_state: 48            # Reasonable state dimension
  d_conv: 4
  expand: 2
  n_layers: 8            # Good depth without memory issues
  dropout: 0.1
  
  mdm_hidden_dim: 384    # Reasonable hidden size
  ntp_hidden_dim: 192
  
  gradient_checkpointing: true   # Enable to save memory
  mixed_precision: true          # Essential for memory efficiency
  
training:
  num_epochs: 100
  learning_rate: 0.001    # Standard learning rate
  weight_decay: 0.01
  warmup_steps: 1000
  
  optimizer: "adamw"
  adam_beta1: 0.9
  adam_beta2: 0.95
  adam_epsilon: 0.00000001
  
  scheduler: "cosine"
  min_lr: 0.00001
  
  mdm_weight: 1.0
  ntp_weight: 0.5
  
  initial_mask_rate: 0.15
  intermediate_mask_rate: 0.30
  final_mask_rate: 0.50
  mask_schedule_epochs: [25, 50]
  
  # Conservative gradient accumulation
  accumulation_steps: 8   # 512 * 8 = 4096 effective batch (8x larger than original!)
  amp_enabled: true
  
  label_smoothing: 0.1
  gradient_clip_val: 1.0
  
  save_every_n_epochs: 10
  checkpoint_dir: "checkpoints"
  keep_last_n_checkpoints: 2
  
evaluation:
  metrics: ["accuracy", "f1", "precision", "recall"]
  distance_threshold: 20.0
  
logging:
  log_every_n_steps: 100
  tensorboard_dir: "logs/tensorboard"
  wandb_project: null
  wandb_entity: null
  
hardware:
  gpu_memory_limit: 12000
  num_gpus: 1
  
seed: 42
deterministic: false
